---
title: "Prediction on Titanic Survivors"
author: "Andr√© Jakob"
date: "17/06/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
#loading packages
library(readxl)
library(readr)
library(tidyverse)
library(caret)
library(knitr)
library(dplyr)
library(rpart)
library(rpart.plot)
library(randomForest)

#loading and taking a first look on the data
titanic0<-read_xls("titanic3.xls")
class(titanic0)
head(titanic0)
names(titanic0)
dim(titanic0)
summary(titanic0)
#looking for NAs
nas0<-data.frame(pclass=sum(is.na(titanic0$pclass)),survived=sum(is.na(titanic0$survived)),sex=sum(is.na(titanic0$sex)),age=sum(is.na(titanic0$age)),sibsp=sum(is.na(titanic0$sibsp)),parch=sum(is.na(titanic0$parch)), fare=sum(is.na(titanic0$fare)), embarked=sum(is.na(titanic0$embarked)),cabin=sum(is.na(titanic0$cabin)),boat=sum(is.na(titanic0$boat)))
kable(nas0)
#selecting usefull variables
titanic<-titanic0%>%select(pclass,survived,sex,age,sibsp,parch,fare,embarked)

#understanding the distribution of each variable
histogram(titanic$pclass)
summary<-data.frame(survived=sum(titanic$survived==1),non_survived=sum(titanic$survived==0))
kable(summary)
sex<-data.frame(Male=sum(titanic$sex=="male"),Female=sum(titanic$sex=="female"))
kable(sex)

nas<-data.frame(pclass=sum(is.na(titanic$pclass)),survived=sum(is.na(titanic$survived)),sex=sum(is.na(titanic$sex)),age=sum(is.na(titanic$age)),sibsp=sum(is.na(titanic$sibsp)),parch=sum(is.na(titanic$parch)), fare=sum(is.na(titanic$fare)), embarked=sum(is.na(titanic$embarked)))
kable(nas)               

mean(titanic$sibsp)
mean(titanic$parch)
#creating variable "family"
titanic$family<-titanic$sibsp+titanic$parch+1
histogram(titanic$family)

#viewing and correcting the NA on "fare"
titanic[is.na(titanic$fare),]
titanic$fare[is.na(titanic$fare)]<-median(titanic$pclass==3&titanic$embarked=="S", na.rm=TRUE)
sum(is.na(titanic$fare))
histogram(titanic$fare)

#viewing the NA on "embarked"
titanic[is.na(titanic$embarked),]
#filtering the data by similar observations on fare, class and family. Other passagers with the same characteristics have embarked in Queenstown port. So I will correct the 2 NAs with Q in the column embarked.
embarked<-titanic%>%filter(fare>=79&fare<=80&pclass=="1"&family=="1")
kable(embarked)
titanic$embarked[is.na(titanic$embarked)]<-"C"

sum(is.na(titanic$embarked))

ports<-data.frame("South Hampton"=(sum(titanic$embarked=="S")),Cherbourg=(sum(titanic$embarked=="C")),Queenstown=(sum(titanic$embarked=="Q")))
kable(ports)

#Using rpart function to predict age from all other variables from the non-NAs (except survived, which is the final goal). Then using predict function to fill the NAs.
names(titanic)
fit_age<-rpart(age~pclass+sex+sibsp+parch+fare+embarked+family,data=titanic[!is.na(titanic$age),], method="anova")
titanic$age[is.na(titanic$age)]<-predict(fit_age,titanic[is.na(titanic$age),])
sum(is.na(titanic$age))
histogram(titanic$age)

#transforming the data as factors before splitting it
titanic$embarked<-as.factor(titanic$embarked)
titanic$sex<-as.factor(titanic$sex)

#creating data partition and dividing the data into training and test set. It was common in the course using a proportion of 0.1 between training and test set. But here the entire data set is not so large (1309 rows). So I choosed a higher proportion of 0.3.

set.seed(1) # I recommend setting the seed into 1 to match yours to my results

test_index <- createDataPartition(y = titanic$survived, times = 1, p = 0.3, list = FALSE)

train <- titanic[-test_index,]
dim(train)
test0 <- titanic[test_index,]
dim(test0)
traintest<-data.frame(test=dim(test0),train=dim(train))
kable(traintest[1,])
#then removed the survived column from the test set.
test<-test0[,-2]
head(test)

#I again made a fit modell using all variables available, this time to predict the data survived.   
fit_survived<-rpart(survived~pclass+sex+sibsp+parch+fare+embarked+family,data=train,method="class")
#Here is a regression tree modell.
prp(fit_survived)

#if we plot the modell, we see how have R decided for the amount of braces of the regression tree. It has reached a total amont of 8 braces, but it is possible to see that from 4 braces on the relative error has not decreased consederably.
plotcp(fit_survived)

#now using random forest, which is basically a combination of multiples regression trees, crossing multiple combinations.
set.seed(1) #again, better set seed to 1 to match my results
rf<-randomForest(as.factor(survived)~pclass+age+sex+sibsp+parch+fare+embarked+family,data=train)

#in the plot below we checked the errors from the random forest algorithm, in red for the non survivors, green for the survivors and black for the central line combining both errors. By this plot it is possible to say that our model predict better the destiny from those who have not survived than for those who did. It is explained because there are more people who have not survived the Titanic, therefore the data available for the non survivors is larger, turning the prediction for them more accurate.
plot(rf,ylim=c(0,0.8))
legend('topright',colnames(rf$err.rate),col=1:3,fill=1:3)

#minimum error table
error<-data.frame(combined_error=min(rf$err.rate[,1]),non_survivor=min(rf$err.rate[,2]),survivor=min(rf$err.rate[,3]))
kable(error)



```

## Introduction
This is the final Report in order to get the Data Science Professional Certificate provided by EdX and HarvardX. After studying the mutiple data science and machine learning proccesses, this final work proposal was to choose a data set by my own and to write a report. Following the reccomentadion, I have researched a couple of data sets on kaggle, and finally downloaded the Titanic data set - whose original file can be found in https://www.kaggle.com/mysticvalley/titanic-original#titanic3.xls. The challenge here is to predict who survived and who did not based on other data available from the Titanic passangers.
Since this is evaluated by other students, I would like to ask the evaluators to be kind with my grammatical errors, because english is not my native language. So I thank you in advance, both for the patience and the evaluation itself. All files mentioned or used are available in the Github link created for these purpose: 

##Method

### Cleaning the data set

The first step of the analysis is to take a look on the data. It was downloaded as a xls file, which I put in a R vector. Some of the numeric observations was originally imputed as characters, but R has coerced those automatically to numeric.


```{r overview dimmension, echo=TRUE}
dim(titanic)
```

A first look on the data set shows us that we have 1309 observations, in other words passangers, and 14 variables: "pclass", "survived", "name",  "sex", "age", "sibsp", "parch", "ticket", "fare", "cabin", "embarked", "boat", "body" and "home.dest". Not all of these variables seem usefull to base the prediction, so I selected only 8 who seem meaningfull. The variables elected as non important are: name, cabin, boat, body and destination. I discarded name and destination because sounds obvious to me that these variables could not interfere in the survival rate of the passager. Cabin and boat could have been usefull, but unfortunately there are too many NA's in their columns - even more than actual data (see below), so it would be too risky infere their values. And the classification of body is already a sign that the person did not survive (even though there are many NA's between the non survivals). There is some subjectiveness in my choices, but I do not see them as very controversial and believe my explanation above is enough.

```{r table NAs per variable, echo=TRUE}
kable(nas0)
```

### Variables

So at the end we have a data set with 8 variables, which I briefly explain:

#### Pclass 
Referes to the class of the passagers, it has 3 possible values (1, 2 or 3).

```{r histogram pclass}
histogram(titanic$pclass)
```

#### Survived 
Is the goal of prediction, and has 2 possible outcomes (1 for survived and 0 for not survived).

```{r histogram survived}
kable(summary)
```

#### Sex
Male or female. 

```{r table sex}
kable(sex)
```

#### Age 
Is a continuous distribution of the age in years from each passager. As showed above, there are 263 NAs on the data set, so I will not plot this distribution nor analyse it further at this moment. 

#### Sibsp and Parch
I considered both as one since they are similar. The first one is the number of brothers, sisters or spouses while the second one is the number of parents and children each passanger has on board. At this point I created a new variable "family" to compute the size of the families on board, which is the sum of "parch" and "sibsp" plus 1 (the passager himself). 

```{r family}
histogram(titanic$family)
```

#### Fare
Is the price each passager paid for the ticket (probably in english pounds). It varies depending on many factors, most of them accordingly to the class of the ticket and the port that the passanger embarked because, among other issues, of the distance to be traveled. So I used the median of the fares of the other passengers with the same class and port to correct this NA. Then I reached the distribution of fares.

```{r fares and NA}
histogram(titanic$fare)
```

#### Embarked
Is the port used by the passanger to embark on Titanic. Ic has 3 options (S for South Hambton, C for Cherbourg and Q for Queenstown). Filtering the data by similar observations on fare, class and family, I founded that Other passagers with the same characteristics have embarked in Queenstown port. So I have corrected the 2 NAs with "C" in the column embarked.

```{r filtering similar embarked}
embarked<-titanic%>%filter(fare>=79&fare<=80&pclass=="1"&family=="1")
kable(embarked)
```

#### Summary of Variables
After filling this NAs, the distribution of the passagers by port is the table below.

```{r embarked and NAs}
kable(ports)
```

### First minor Machine Learning

As it is showed above, most of the variables have been cleaned and are ready for the prediction. There is though the column Age with several NAs in it (exactly 263). Unfortunately this seems to be an important criteria for survival and I can not give up on this. Since age usually follows a normal distribution, I could have been corrected the NA values with a median of the non NAs values. Nevertheless, I choosed to run a first minor Machine Learning experiment to fill the NAs. Using the Regression Trees technique I created a fit modell function to predict age from all other variables from the non-NAs (except survived, which is the final goal). Then I used predict function to fill the NAs, which gave me a age distribution that I show below.

```{r Age and NAs}
histogram(titanic$age)
```

### Splitting train and test set

Now that the data set is clean, I divided it into a training and a test set. It was common during the course to stablish a proportion of 0.1 between training and test set. But here the entire data set is not so large (1309 rows), so I choosed a higher proportion of 0.3. After separating both sets, I removed the "survived" color from the test set, in order to predict this information based on the training set.

```{r dim train and test set}
kable(traintest[1,])

```

### Regression Tree

Again I used the Regression Trees technique I created a fit modell function to predict if the passanger has or has not survived from all other variables. Below I plotted the tree to show an example on whow are the questions used by R for prediction.

```{r regression tree}
#I again made a fit modell using all variables available, this time to predict the data survived.   
fit_survived<-rpart(survived~pclass+sex+sibsp+age+parch+fare+embarked+family,data=train,method="class")

prp(fit_survived)
```

If the modell is plotted,it is possible to see how have R decided for the amount of braces of the regression tree. It has reached a total amont of 8 braces, but from 4 braces on the relative error has not decreased consederably.

``` {r braces of regression tree}
plotcp(fit_survived)
```

### Random Forest

So I used the Random Forest technique, which is basically a combination of multiples regression trees, crossing multiple combinations. In the plot below we checked the errors from the random forest algorithm, in red for the non survivors, green for the survivors and black for the central line combining both errors. By this plot it is possible to say that our model predict better the destiny from those who have not survived than for those who did. It is explained because there are more people who have not survived the Titanic than otherwise, therefore the data available for the non survivors is larger, turning the prediction for them more accurate.

``` {r Random Forest errors}
plot(rf,ylim=c(0,0.8))
legend('topright',colnames(rf$err.rate),col=1:3,fill=1:3)
```

## Results

Below there is a table showing the minimum error reached for each outcome: 0.10 for non survivors and 0.33 for survivors, with a combined error of 0.19.

```{r minimum error}
kable(error)
```

## Conclusion

At first view, the combined minimum error of 0.19 may not appear much impressive. I sustain otherwise, since one must keep in mind the characteristics of the data set. The provided information is general observations such Sex, Age, family on board, fare paid, class and port, which means a considerably little amount of data about the passagers of Titanic, which in its turn is also a relativ little data set. Even though the Random Forest technique made possible to predict their destiny with satisfatory accuracy. A combined minimum error of 0.19 is relatively good prediction, and the prediction for non survivors can be classified as excelent: just 0.10. 

